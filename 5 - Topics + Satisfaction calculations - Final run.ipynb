{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0248b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09e0fc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mlogt\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n",
      "C:\\Users\\mlogt\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"final_df2.xlsx\",index_col=0)\n",
    "df.dropna(axis=0,inplace=True)\n",
    "data = list(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7f81b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple preprocess\n",
    "to_df = {} #Initiate temp dict\n",
    "for i in range(len(df)): #Loop through each row\n",
    "    to_df[i] = simple_preprocess(str(df.loc[i,'review']), deacc=True) #Simple preprocess implementation\n",
    "df['review'] = to_df.values() #Copy the data in temp dict to DataFrame\n",
    "#Transform data to list and make dictionary (dictionary contains each occuring word)\n",
    "data = list(df['review']) #Transform data to list\n",
    "id2word = corpora.Dictionary(data) # Make dictionary, where each word links to an id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "515cb883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "train, test = train_test_split(data,train_size=0.8,random_state=3099) #Split data\n",
    "train_corpus = [] #Initiate train corpus, corpus converts words into tuples (combi of word and frequency of that word)\n",
    "for review in train: #Loop through train data, transform each review to ids (corresponding to the dictionairy) and add to the train corpus\n",
    "    new = id2word.doc2bow(review)\n",
    "    train_corpus.append(new) \n",
    "test_corpus = [] #Initiate test corpus, corpus converts words into tuples (combi of word and frequency of that word)\n",
    "for review in test: #Loop through test data, transform each review to ids (corresponding to the dictionairy) and add to the test corpus\n",
    "    new = id2word.doc2bow(review)\n",
    "    test_corpus.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b2b141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Best model (has 6 topics, 7 passes)\n",
    "lda_model = gensim.models.ldamulticore.LdaMulticore(workers=5,\n",
    "                                                    corpus=train_corpus,\n",
    "                                                    id2word=id2word,\n",
    "                                                    num_topics=6,\n",
    "                                                    alpha=0.01,\n",
    "                                                    eta = 0.1,\n",
    "                                                    passes=7,\n",
    "                                                    random_state=3099)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5062c729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews processed: 0\n",
      "Number of reviews processed: 10000\n",
      "Number of reviews processed: 20000\n",
      "Number of reviews processed: 30000\n",
      "Number of reviews processed: 40000\n",
      "Number of reviews processed: 50000\n"
     ]
    }
   ],
   "source": [
    "#Reset index and drop old index\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "df.drop('index',axis=1,inplace=True)\n",
    "\n",
    "#Get topic probabilities for each review\n",
    "dict_0 = dict.fromkeys(range(len(df)), 0) #Initiate temp dicts with 0's for the length of the number of reviews for each topic\n",
    "dict_1 = dict.fromkeys(range(len(df)), 0)\n",
    "dict_2 = dict.fromkeys(range(len(df)), 0)\n",
    "dict_3 = dict.fromkeys(range(len(df)), 0)\n",
    "dict_4 = dict.fromkeys(range(len(df)), 0)\n",
    "dict_5 = dict.fromkeys(range(len(df)), 0)\n",
    "for idx, i in enumerate(df['review']): #Loop through each review\n",
    "    prob = lda_model.get_document_topics(id2word.doc2bow(list(simple_preprocess(str(i), deacc=True)))) #Calculate topic probabilities of that review\n",
    "    for j in prob: #For each topic probability\n",
    "        if j[0] == 0: #If it corresponds to appropiate topic, copy probability to temp dict\n",
    "            dict_0[idx] = j[1]\n",
    "        if j[0] == 1:\n",
    "            dict_1[idx] = j[1]\n",
    "        if j[0] == 2:\n",
    "            dict_2[idx] = j[1]\n",
    "        if j[0] == 3:\n",
    "            dict_3[idx] = j[1]\n",
    "        if j[0] == 4:\n",
    "            dict_4[idx] = j[1]\n",
    "        if j[0] == 5:\n",
    "            dict_5[idx] = j[1]\n",
    "    if idx % 10000 == 0: #Print an update after every 10,000 reviews\n",
    "        print(\"Number of reviews processed:\",idx)\n",
    "df['prob_1'] = dict_0.values() #Copy data from temp dict to column in DataFrame\n",
    "df['prob_2'] = dict_1.values()\n",
    "df['prob_3'] = dict_2.values()\n",
    "df['prob_4'] = dict_3.values()\n",
    "df['prob_5'] = dict_4.values()\n",
    "df['prob_6'] = dict_5.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6e39c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average satisfaction for topic 1  : 6.90688662604226\n",
      "Average satisfaction for topic 2  : 6.925187014090497\n",
      "Average satisfaction for topic 3  : 8.876486213118651\n",
      "Average satisfaction for topic 4  : 6.418822699577366\n",
      "Average satisfaction for topic 5  : 8.074445468123207\n",
      "Average satisfaction for topic 6  : 8.985389859560115\n"
     ]
    }
   ],
   "source": [
    "#Calculate average satisfaction ratings\n",
    "for topic in range(1,7): #Loop through each topic\n",
    "    tot = 0 #Initiate running total\n",
    "    for i in range(len(df)): #Loop through each review\n",
    "        tot += df.loc[i,'prob_'+str(topic)]*df.loc[i,'avg_rating'] #Add the product of the probability of each topic and the average rating of each review to the running total\n",
    "    print(\"Average satisfaction for topic\",topic,\" :\",tot / sum(df['prob_'+str(topic)])) #Calculate avg (running total divided by the sum of probabilities for that topic) and print this avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2273aa32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Export all keyword probabilities\n",
    "for i in range(0,6): #Loop through each topic\n",
    "    df2 = pd.DataFrame(lda_model.show_topic(i,50)) #Calculate dataframe with keyword probabilities\n",
    "    df2.to_excel('keywords_topic'+str(i+1)+'.xlsx') #Export to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fe3eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classify each review with appropiate topic\n",
    "df.drop('level_0',axis=1,inplace=True) #Drop irrelevant column\n",
    "to_df = {} #Initiate temp dict\n",
    "for i in range(len(df)): #Loop through each review\n",
    "    l = list(df.iloc[i,3:,]) #Initiate temp list with the topic probabilities of a review\n",
    "    to_df[i] = l.index(max(l)) + 1 #Take the maximum of temp list and assign to temp dict\n",
    "df['topic'] = to_df.values() #Copy values in temp dict to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70bd3954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export probability dataframe to Excel\n",
    "df.to_excel('probs.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4e65a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    28911\n",
       "4     8807\n",
       "3     7931\n",
       "1     6429\n",
       "5     5441\n",
       "2     1275\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the number of reviews assigned to each topic\n",
    "df['topic'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
